# Configuration for a quick dry run to test the training pipeline

model_config:
  # Architecture (Reduced Size)
  hidden_size: 128
  num_hidden_layers: 2
  num_attention_heads: 4
  intermediate_size: 256
  vocab_size: 50272 # Padded vocab size (divisible by 16) for FP8 compatibility

  # PEER configuration (Simplified)
  use_peer: true
  peer_start_layer: 1 # Start PEER earlier for testing
  peer_config:
    num_experts: 64  # Reduced number of experts
    num_experts_per_tok: 4 # Reduced experts per token
    num_heads: 2 # Reduced selection heads
    expert_hidden_size: 8 # Can keep 1 or use slightly larger for debug
    product_key_dim: [8, 8] # Smaller dimensions matching num_experts
    query_dim: 32 # Smaller query dimension
    batch_norm_query: false # Disable BN for simplicity
    log_expert_usage: true # Keep logging enabled
    log_freq: 10 # Log expert usage frequently
    usage_threshold: 5.0

  # MLA configuration (Simplified)
  mla_config:
    q_lora_rank: 32 # Reduced LoRA ranks
    kv_lora_rank: 16
    qk_rope_head_dim: 16 # Adjusted RoPE dimension (8 -> 16) to make kv_lora_rank + qk_rope_head_dim divisible by 16 (16+16=32)
    v_head_dim: 16 # Reduced Value head dimension
    qk_nope_head_dim: 8 # Reduced NoPE dimension

train_config:
  output_dir: "./output_dry_run" # Separate output directory
  per_device_train_batch_size: 4 # Small batch size
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1 # No accumulation
  learning_rate: 1e-4 # Slightly higher LR for quick check
  weight_decay: 0.01
  max_grad_norm: 1.0
  # num_train_epochs: null # Use max_steps instead
  max_steps: 5000 # Run for 5000 steps to observe learning
  lr_scheduler_type: "cosine" # Use cosine scheduler
  warmup_ratio: 0.1 # Use 10% warmup
  log_level: "info" # Use "info" or "debug"
  logging_steps: 10 # Log frequently
  save_steps: 100000 # Effectively disable saving
  eval_steps: 100000 # Effectively disable evaluation
  seed: 42
  fp16: true # Use mixed precision if desired
  bf16: false
  tf32: false
  resume_from_checkpoint: null

dataset_config:
  datasets:
    # Use a small, standard dataset like wikitext-2
    - name: "wikitext2_dry_run"
      path: "wikitext"
      subset: "wikitext-2-raw-v1" # Specify the small subset
      split: "train"
      streaming: false # Disable streaming for small dataset
      weight: 1.0
      text_field: "text"
  tokenizer_name: "gpt2" # Keep tokenizer consistent
  max_seq_length: 256 # Shorter sequence length

eval_config:
  # Evaluation is disabled via eval_steps, but keep structure
  evals_registry_path: "./evals/registry"
  evals:
    - "hellaswag"
  eval_batch_size: 4

wandb_config: null # Disable wandb logging for dry run
