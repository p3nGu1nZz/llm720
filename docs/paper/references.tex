@article{GShard2020,
  author    = {Dmitry Lepikhin and others},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  journal   = {ArXiv},
  year      = {2020},
  volume    = {abs/2006.16668},
  url       = {https://arxiv.org/abs/2006.16668}
}

@article{SwitchTransformer2021,
  author    = {William Fedus and others},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal   = {ArXiv},
  year      = {2021},
  volume    = {abs/2101.03961},
  url       = {https://arxiv.org/abs/2101.03961}
}

@article{DSv32024,
  author    = {DeepSeek-AI et al.},
  title     = {DeepSeek-V3 Technical Report},
  journal   = {ArXiv},
  year      = {2024},
  volume    = {abs/2412.19437},
  url       = {https://arxiv.org/abs/2412.19437}
}

@article{MillionExperts2024,
  author    = {Xu Owen He},
  title     = {Mixture of a Million Experts},
  journal   = {ArXiv},
  year      = {2024},
  volume    = {abs/2407.04153},
  url       = {https://arxiv.org/abs/2407.04153}
}

@article{BeyondChinchilla2024,
  author    = {Nikhil Sardana and others},
  title     = {Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
  journal   = {ArXiv},
  year      = {2024},
  volume    = {abs/2401.00448},
  url       = {https://arxiv.org/abs/2401.00448}
}

@article{OriginalMoE1991,
  author    = {Robert A. Jacobs and Michael I. Jordan},
  title     = {Adaptive Mixture of Local Experts},
  journal   = {Neural Computation},
  year      = {1991},
  volume    = {3},
  number    = {1},
  pages     = {79--87},
  doi       = {10.1162/neco.1991.3.1.79}
}

@article{SparselyGatedMoE2017,
  author    = {Noam Shazeer and others},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  journal   = {ArXiv},
  year      = {2017},
  volume    = {abs/1701.06538},
  url       = {https://arxiv.org/abs/1701.06538}
}

@article{VMoE2021,
  author    = {Carlos Riquelme and others},
  title     = {Scaling Vision with Sparse Mixture of Experts},
  journal   = {ArXiv},
  year      = {2021},
  volume    = {abs/2106.05974},
  url       = {https://arxiv.org/abs/2106.05974}
}

@article{BASE2021,
  author    = {Mike Lewis and others},
  title     = {BASE Layers: Simplifying Training and Inference of Large-Scale Models},
  journal   = {ArXiv},
  year      = {2021},
  volume    = {abs/2103.16716},
  url       = {https://arxiv.org/abs/2103.16716}
}

@article{ExpertChoiceRouting2022,
  author    = {Yanqi Zhou and others},
  title     = {Expert Choice Routing for Scalable Mixture-of-Experts Models},
  journal   = {ArXiv},
  year      = {2022},
  volume    = {abs/2202.09368},
  url       = {https://arxiv.org/abs/2202.09368}
}

@article{fMOE2025,
  author    = {Hanfei Yu and others},
  title     = {fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving},
  journal   = {ArXiv},
  year      = {2025},
  volume    = {abs/2502.05370},
  url       = {https://arxiv.org/abs/2502.05370}
}

@article{ProMoE2024,
  author    = {Song et al.},
  title     = {ProMoE: Efficient Mixture of Experts with Prefetching},
  journal   = {ArXiv},
  year      = {2024},
  volume    = {abs/2410.22134},
  url       = {https://arxiv.org/abs/2410.22134}
}

@article{FineGrainedScaling2024,
  author    = {Krajewski et al.},
  title     = {Fine-Grained Mixture-of-Experts Scaling Laws},
  journal   = {ArXiv},
  year      = {2024},
  volume    = {abs/2402.07871},
  url       = {https://arxiv.org/abs/2402.07871}
}

@article{MoEMemoryEfficiency2025,
  author    = {Ludziejewski et al.},
  title     = {Memory-Efficient Mixture-of-Experts Training: Theoretical Insights},
  journal   = {ArXiv},
  year      = {2025},
  volume    = {abs/2502.05172},
  url       = {https://arxiv.org/abs/2502.05172}
}

@article{LLM3602023,
  author    = {Lieu et al.},
  title     = {LLM360: Open-Source End-to-End Large Model Development},
  journal   = {ArXiv},
  year      = {2023},
  volume    = {abs/2401.00448},
  url       = {https://arxiv.org/abs/2401.00448}
}